{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XXqiG6cyh85LbOltOQT4jC-bcdRue8Z1",
      "authorship_tag": "ABX9TyOs75cAa104aMjYOH2Trn9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtyomGrachev/defeater/blob/ArtyomGrachev-PointNet-DeepCosineMetric-v1/PointNet_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxQQSMDJI3W-",
        "colab_type": "text"
      },
      "source": [
        "### Creates pytorch-style Waymo-OD dataset for 3D point classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2kyLYbxlV5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!apt-get install tree\n",
        "!pip install importlib\n",
        "!pip install torchvision==0.4.0\n",
        "!pip install  Pillow==6.2.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDapRH6jcoc",
        "colab_type": "code",
        "outputId": "1d37a6df-f6ee-4552-8805-6f68045ad832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Clone github Waymo-od repo with util functions\n",
        "\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!pip install waymo-open-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'waymo-od'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/82)\u001b[K\rremote: Counting objects:   2% (2/82)\u001b[K\rremote: Counting objects:   3% (3/82)\u001b[K\rremote: Counting objects:   4% (4/82)\u001b[K\rremote: Counting objects:   6% (5/82)\u001b[K\rremote: Counting objects:   7% (6/82)\u001b[K\rremote: Counting objects:   8% (7/82)\u001b[K\rremote: Counting objects:   9% (8/82)\u001b[K\rremote: Counting objects:  10% (9/82)\u001b[K\rremote: Counting objects:  12% (10/82)\u001b[K\rremote: Counting objects:  13% (11/82)\u001b[K\rremote: Counting objects:  14% (12/82)\u001b[K\rremote: Counting objects:  15% (13/82)\u001b[K\rremote: Counting objects:  17% (14/82)\u001b[K\rremote: Counting objects:  18% (15/82)\u001b[K\rremote: Counting objects:  19% (16/82)\u001b[K\rremote: Counting objects:  20% (17/82)\u001b[K\rremote: Counting objects:  21% (18/82)\u001b[K\rremote: Counting objects:  23% (19/82)\u001b[K\rremote: Counting objects:  24% (20/82)\u001b[K\rremote: Counting objects:  25% (21/82)\u001b[K\rremote: Counting objects:  26% (22/82)\u001b[K\rremote: Counting objects:  28% (23/82)\u001b[K\rremote: Counting objects:  29% (24/82)\u001b[K\rremote: Counting objects:  30% (25/82)\u001b[K\rremote: Counting objects:  31% (26/82)\u001b[K\rremote: Counting objects:  32% (27/82)\u001b[K\rremote: Counting objects:  34% (28/82)\u001b[K\rremote: Counting objects:  35% (29/82)\u001b[K\rremote: Counting objects:  36% (30/82)\u001b[K\rremote: Counting objects:  37% (31/82)\u001b[K\rremote: Counting objects:  39% (32/82)\u001b[K\rremote: Counting objects:  40% (33/82)\u001b[K\rremote: Counting objects:  41% (34/82)\u001b[K\rremote: Counting objects:  42% (35/82)\u001b[K\rremote: Counting objects:  43% (36/82)\u001b[K\rremote: Counting objects:  45% (37/82)\u001b[K\rremote: Counting objects:  46% (38/82)\u001b[K\rremote: Counting objects:  47% (39/82)\u001b[K\rremote: Counting objects:  48% (40/82)\u001b[K\rremote: Counting objects:  50% (41/82)\u001b[K\rremote: Counting objects:  51% (42/82)\u001b[K\rremote: Counting objects:  52% (43/82)\u001b[K\rremote: Counting objects:  53% (44/82)\u001b[K\rremote: Counting objects:  54% (45/82)\u001b[K\rremote: Counting objects:  56% (46/82)\u001b[K\rremote: Counting objects:  57% (47/82)\u001b[K\rremote: Counting objects:  58% (48/82)\u001b[K\rremote: Counting objects:  59% (49/82)\u001b[K\rremote: Counting objects:  60% (50/82)\u001b[K\rremote: Counting objects:  62% (51/82)\u001b[K\rremote: Counting objects:  63% (52/82)\u001b[K\rremote: Counting objects:  64% (53/82)\u001b[K\rremote: Counting objects:  65% (54/82)\u001b[K\rremote: Counting objects:  67% (55/82)\u001b[K\rremote: Counting objects:  68% (56/82)\u001b[K\rremote: Counting objects:  69% (57/82)\u001b[K\rremote: Counting objects:  70% (58/82)\u001b[K\rremote: Counting objects:  71% (59/82)\u001b[K\rremote: Counting objects:  73% (60/82)\u001b[K\rremote: Counting objects:  74% (61/82)\u001b[K\rremote: Counting objects:  75% (62/82)\u001b[K\rremote: Counting objects:  76% (63/82)\u001b[K\rremote: Counting objects:  78% (64/82)\u001b[K\rremote: Counting objects:  79% (65/82)\u001b[K\rremote: Counting objects:  80% (66/82)\u001b[K\rremote: Counting objects:  81% (67/82)\u001b[K\rremote: Counting objects:  82% (68/82)\u001b[K\rremote: Counting objects:  84% (69/82)\u001b[K\rremote: Counting objects:  85% (70/82)\u001b[K\rremote: Counting objects:  86% (71/82)\u001b[K\rremote: Counting objects:  87% (72/82)\u001b[K\rremote: Counting objects:  89% (73/82)\u001b[K\rremote: Counting objects:  90% (74/82)\u001b[K\rremote: Counting objects:  91% (75/82)\u001b[K\rremote: Counting objects:  92% (76/82)\u001b[K\rremote: Counting objects:  93% (77/82)\u001b[K\rremote: Counting objects:  95% (78/82)\u001b[K\rremote: Counting objects:  96% (79/82)\u001b[K\rremote: Counting objects:  97% (80/82)\u001b[K\rremote: Counting objects:  98% (81/82)\u001b[K\rremote: Counting objects: 100% (82/82)\u001b[K\rremote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 700 (delta 34), reused 39 (delta 14), pack-reused 618\u001b[K\n",
            "Receiving objects: 100% (700/700), 14.17 MiB | 34.22 MiB/s, done.\n",
            "Resolving deltas: 100% (417/417), done.\n",
            "Collecting waymo-open-dataset\n",
            "  Downloading waymo_open_dataset-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=1.14.0 in /tensorflow-1.15.0/python3.6 (from waymo-open-dataset) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.18.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.0/python3.6 (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.0/python3.6 (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.24.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.1.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.14.0->waymo-open-dataset) (45.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14.0->waymo-open-dataset) (2.8.0)\n",
            "Installing collected packages: waymo-open-dataset\n",
            "Successfully installed waymo-open-dataset-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0oM54zvjbUy",
        "colab_type": "code",
        "outputId": "3468df12-b301-4266-9a29-473379966c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# required libs\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import inspect\n",
        "import tarfile\n",
        "import tqdm\n",
        "\n",
        "import random\n",
        "import math\n",
        "import itertools\n",
        "import functools\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.utils.data as tdata\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset.utils import  box_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py:59: The name tf.unsorted_segment_max is deprecated. Please use tf.math.unsorted_segment_max instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py:226: The name tf.unsorted_segment_min is deprecated. Please use tf.math.unsorted_segment_min instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl5NK87yhyaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment to download train segment from Waymo google cloud into google drive\n",
        "# Training sets: training_0000.tar - training_0031.tar\n",
        "# Validation sets: validation_0000.tar - validation_0007.tar\n",
        "\n",
        "#!gsutil cp gs://waymo_open_dataset_v_1_0_0/training/training_0001.tar /content/drive/'My Drive'/Waymo_OD/\n",
        "\n",
        "def extract_from_tar(extract_from, extract_to):\n",
        "  file_tar = tarfile.open(name=extract_from, mode='r', fileobj=None, bufsize=10240)\"\n",
        "  file_tar.extractall(path=extract_to)\n",
        "\n",
        "\n",
        "#os.remove(r\"/content/drive/My Drive/Waymo_OD/training_0001.tar\")\n",
        "#os.remove(r\"/content/drive/My Drive/Waymo_OD/LICENSE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nu69FbQhv1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PointCloudParser(object):\n",
        "  \"\"\"Performs point cloud 3D-box filtering for a given set of classes.\"\"\"\n",
        "\n",
        "  def __init__(self, data_path, max_segments=None, cls_to_filter=(\"TYPE_PEDESTRIAN\", \"TYPE_VEHICLE\")):\n",
        "    \"\"\"\n",
        "    data_path: Data path to folder that containts Waymo TFRecords\n",
        "\n",
        "    max_segments: number of segments to process (None eq. all segments)\n",
        "\n",
        "    cls_to_filter: point clouds for these classes will be extracted\n",
        "    \"\"\"\n",
        "\n",
        "    assert os.path.exists(data_path), \"Existing path is required\"\n",
        "    assert np.all([os.path.splitext(file_name)[-1] == \".tfrecord\" \n",
        "                   for file_name in os.listdir(data_path)]), \"All files in the folder should be .tfrecord\"\n",
        "\n",
        "    self.data_path = data_path\n",
        "\n",
        "    self.cls_to_filter = cls_to_filter\n",
        "\n",
        "    self.segments_to_proc = os.listdir(self.data_path)[:max_segments]\n",
        "\n",
        "    self.type_enum = {\n",
        "                        \"TYPE_UNKNOWN\" : 0,\n",
        "                        \"TYPE_VEHICLE\" : 1,\n",
        "                        \"TYPE_PEDESTRIAN\" : 2,\n",
        "                        \"TYPE_SIGN\" : 3,\n",
        "                        \"TYPE_CYCLIST\" : 4\n",
        "                     }\n",
        "\n",
        "    self.code_2_type = {v:k for k,v in self.type_enum.items()}\n",
        "\n",
        "    self.obj_codes = tuple(self.type_enum[ctf] for ctf in self.cls_to_filter)\n",
        "  \n",
        "    self.bbox_count_dict = collections.defaultdict(int)\n",
        "\n",
        "  def start_processing(self, dest_path):\n",
        "    \"\"\"\n",
        "    Main routine method that starts points filtering. \n",
        "    Creates len(self.obj_codes) number folders in dest_path \n",
        "    for each class and saves point clouds for each box in .npy binary file. \n",
        "    \"\"\"\n",
        "\n",
        "    self.check_dir(dest_path)\n",
        "    self.dest_path = dest_path\n",
        "\n",
        "    for segment_name in self.segments_to_proc:\n",
        "      dataset = tf.data.TFRecordDataset(\n",
        "          os.path.join(self.data_path, segment_name), compression_type=\"\")\n",
        "      self.filter_frame_points(dataset)\n",
        "      \n",
        "\n",
        "  def filter_frame_points(self, dataset, min_pts_threshold=75):\n",
        "    \"\"\"\n",
        "    Return dict that maps from object type ids to point clouds. \n",
        "    Value for each key is a list of numpy arrays, \n",
        "    where each numpy array containts points for a single 3D bbox.\n",
        "    \"\"\"\n",
        "\n",
        "    bbox_dict = {obj:[] for obj in self.obj_codes}\n",
        "\n",
        "    for data in tqdm.tqdm_notebook(dataset):\n",
        "      frame = open_dataset.Frame()\n",
        "      \n",
        "      frame.ParseFromString(bytearray(data.numpy()))\n",
        "\n",
        "      frame_pts = self.frame_points(frame)\n",
        "\n",
        "      for obj_code in self.obj_codes:\n",
        "        boxes = self.frame_boxes_3D(frame, obj_code)\n",
        "\n",
        "        if len(boxes):\n",
        "          bbox_dict[obj_code].extend(\n",
        "              \n",
        "               [\n",
        "                filt_pts for filt_pts in (tf.boolean_mask(frame_pts, mask).numpy() \n",
        "                                          for mask in tf.transpose(\n",
        "                                              box_utils.is_within_box_3d(frame_pts, boxes)\n",
        "                                          )) \n",
        "                if len(filt_pts) >= min_pts_threshold\n",
        "               ]\n",
        "          )\n",
        "    \n",
        "    self.save_point_cloud(bbox_dict)\n",
        "    \n",
        "\n",
        "  def frame_points(self, frame, projection=False):\n",
        "    \"\"\"\n",
        "    Returns Tensor of points in a vehicle coord. system for a given frame. \n",
        "    If projection is True => returns points projection on frame\n",
        "    \"\"\"\n",
        "    (range_images, camera_projections, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
        "\n",
        "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
        "        frame,\n",
        "        range_images,\n",
        "        camera_projections,\n",
        "        range_image_top_pose,\n",
        "        ri_index=0)\n",
        "\n",
        "    if projection:\n",
        "      return tf.constant(np.concatenate(cp_points))\n",
        "    else:\n",
        "      return tf.constant(np.concatenate(points, axis=0))\n",
        "\n",
        "\n",
        "  def frame_boxes_3D(self, frame, obj_code):\n",
        "    \"\"\"\n",
        "    Returns np.array of box descriptors for a given frame\n",
        "    \"\"\"\n",
        "\n",
        "    box_stats = np.array([tf.constant([lable.box.center_x, lable.box.center_y, lable.box.center_z, \n",
        "                  lable.box.width, lable.box.length, lable.box.height, lable.box.heading])\n",
        "                  for lable in frame.laser_labels if lable.type == obj_code])\n",
        "\n",
        "    return box_stats\n",
        "\n",
        "\n",
        "  def check_dir(self, dest_path):\n",
        "    \"\"\"Creates required dir. for files\"\"\"\n",
        "    if not os.path.exists(dest_path):\n",
        "      os.mkdir(dest_path)\n",
        "    for obj_code in self.obj_codes:\n",
        "      obj_folder = os.path.join(dest_path, self.code_2_type[obj_code])\n",
        "      if not os.path.exists(obj_folder):\n",
        "        os.mkdir(obj_folder)\n",
        "\n",
        "\n",
        "  def save_point_cloud(self, bbox_dict):\n",
        "    \"\"\"Saves filtered point cloud (numpy array of size [N, 3]) into .npy binary file.\"\"\"\n",
        "\n",
        "    print(\"Saving.\")\n",
        "    \n",
        "    for obj_code in bbox_dict.keys():\n",
        "      for bbox in bbox_dict[obj_code]:\n",
        "        np.save(\n",
        "                  os.path.join(self.dest_path, self.code_2_type[obj_code], \n",
        "                              \"{0}.npy\".format(self.bbox_count_dict[obj_code]).zfill(12)), \n",
        "                  bbox\n",
        "               )\n",
        "        self.bbox_count_dict[obj_code] += 1\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Y584c9Nhxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Point cloud processing functions\n",
        "# segments_path = \"/content/drive/My Drive/Waymo_OD/\"\n",
        "# testing_pcp = PointCloudParser(segments_path)\n",
        "# testing_pcp.start_processing(\"/content/drive/My Drive/PointNet/Data_folder_train_0001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yra0BHHZcYnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shutil.rmtree(\"/content/drive/My Drive/PointNet/Data_folder_train_0001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWlmN3ejq3N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def box_metadata_parser(dataset, dataset_id, obj_type=\"TYPE_PEDESTRIAN\", save=False):\n",
        "  \"\"\"\n",
        "  Compute box metadata from a given segment.\n",
        "  Box metadata includes box track_id and L2 distance to box in vehicle coord. system\n",
        "  \"\"\"\n",
        "  df_dict_template = {\"track_id\" : [], \"box_dist\" : []}\n",
        "\n",
        "  type_enum = {\n",
        "      \"TYPE_UNKNOWN\" : 0,\n",
        "      \"TYPE_VEHICLE\" : 1,\n",
        "      \"TYPE_PEDESTRIAN\" : 2,\n",
        "      \"TYPE_SIGN\" : 3,\n",
        "      \"TYPE_CYCLIST\" : 4,\n",
        "    }\n",
        "\n",
        "  obj_code = type_enum[obj_type]\n",
        "\n",
        "  for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    for lable in frame.laser_labels:\n",
        "      if lable.type == obj_code:\n",
        "        df_dict_template[\"track_id\"].append(lable.id)\n",
        "        df_dict_template[\"box_dist\"].append(np.linalg.norm([lable.box.center_x, lable.box.center_y, lable.box.center_z]))\n",
        "\n",
        "  metadata_df = pd.DataFrame.from_dict(df_dict_template)\n",
        "\n",
        "  if save:\n",
        "    if not os.path.exists(\"./box_metadata\"):\n",
        "      os.mkdir(\"./box_metadata\")\n",
        "    \n",
        "    metadata_df.to_csv(\".//box_metadata//metadata_{0}_{1}.csv\".format(obj_type, dataset_id), index = False, header=True)\n",
        "\n",
        "  return metadata_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KtOYl7ytcZt",
        "colab_type": "text"
      },
      "source": [
        "### Create data loaders for train and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mkPc734kjAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MinPointSampler(object):\n",
        "  \"\"\"Transfromation that samples pts_num number of points from the input point cloud\"\"\"\n",
        "  def __init__(self, pts_num):\n",
        "    self.pts_num = pts_num\n",
        "\n",
        "  def __call__(self, point_cloud):\n",
        "    return point_cloud[np.random.choice(point_cloud.shape[0], size=self.pts_num, replace=False), :]\n",
        "\n",
        "\n",
        "class PointScaler(object):\n",
        "  \"\"\"Scales point cloud to a new one with mean = 0 and maximum vector length of 1\"\"\"\n",
        "  def __call__(self, point_cloud): \n",
        "    return (point_cloud - np.mean(point_cloud, axis=0)) / np.linalg.norm(point_cloud, axis=1).max()\n",
        "\n",
        "\n",
        "class RndPointsAugmentations(object):\n",
        "  \"\"\"\n",
        "  Performs 2 types of point data augmentation: \n",
        "  Random jittering via uniformly distributed noise and random rotation along z-axis\n",
        "  \"\"\"\n",
        "  def __init__(self, jitter_a=0, jitter_b=0.2):\n",
        "    self.jitter_a = jitter_a\n",
        "    self.jitter_b = jitter_b\n",
        "    \n",
        "  def __call__(self, point_cloud):\n",
        "    theta = np.random.uniform(0, np.pi*2)\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
        "    point_cloud[:, :2] = point_cloud[:, :2].dot(rotation_matrix)\n",
        "    return point_cloud + np.random.normal(self.jitter_a, self.jitter_b, size=point_cloud.shape)\n",
        "\n",
        "\n",
        "def create_train_val_data_loaders(data_dir, *, min_pts=75, batch_size=32, validation_frac = 0.2, num_of_workers=0):\n",
        "    \"\"\"\n",
        "    Return pair of pytorch dataloaders for train and validation sets.\n",
        "    \"\"\"\n",
        "    # sample => scale => (if train) random jitter and random rotation along z axis => transform to Pytorch tensor \n",
        "\n",
        "    mps_transform = MinPointSampler(min_pts)\n",
        "    pt_scaler = PointScaler()\n",
        "    points_aug = RndPointsAugmentations()\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "                                           mps_transform,\n",
        "                                           points_aug,\n",
        "                                           pt_scaler,\n",
        "                                           transforms.ToTensor()\n",
        "                                          ])\n",
        "    \n",
        "    val_transforms = transforms.Compose([\n",
        "                                        mps_transform,\n",
        "                                        pt_scaler,\n",
        "                                        transforms.ToTensor()\n",
        "                                       ])\n",
        "    \n",
        "    train_data = datasets.DatasetFolder(data_dir, loader=np.load, extensions=(\"npy\"), \n",
        "                                        transform=train_transforms)\n",
        "\n",
        "    val_data = datasets.DatasetFolder(data_dir, loader=np.load, extensions=(\"npy\"),\n",
        "                                      transform=val_transforms)\n",
        "        \n",
        "    dataset_len = len(train_data)\n",
        "\n",
        "    indices = np.arange(dataset_len)\n",
        "    \n",
        "    val_abs_size = np.int(np.floor(validation_frac * dataset_len))\n",
        "\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    train_id, val_id = indices[val_abs_size:], indices[:val_abs_size]\n",
        "    \n",
        "    train_sampler = tdata.SubsetRandomSampler(train_id)\n",
        "    \n",
        "    val_sampler = tdata.SubsetRandomSampler(val_id)\n",
        "    \n",
        "    train_loader = tdata.DataLoader(train_data,\n",
        "                   sampler=train_sampler, batch_size=batch_size, num_workers=num_of_workers)\n",
        "    \n",
        "    val_loader = tdata.DataLoader(val_data,\n",
        "                   sampler=val_sampler, batch_size=batch_size, num_workers=num_of_workers)\n",
        "    \n",
        "    return train_loader, val_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp5dG6VCHiA6",
        "colab_type": "text"
      },
      "source": [
        "### PointNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBqxmCKHpxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class STN3d(nn.Module):\n",
        "  \"\"\"TNet for 3D point transformation\"\"\"\n",
        "  def __init__(self):\n",
        "      super(STN3d, self).__init__()\n",
        "      self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.fc3 = nn.Linear(256, 9)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      batchsize = x.size()[0]\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = F.relu(self.bn3(self.conv3(x)))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "\n",
        "      x = F.relu(self.bn4(self.fc1(x)))\n",
        "      x = F.relu(self.bn5(self.fc2(x)))\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
        "      if x.is_cuda:\n",
        "          iden = iden.cuda()\n",
        "      x = x + iden\n",
        "      x = x.view(-1, 3, 3)\n",
        "      return x\n",
        "\n",
        "\n",
        "class STNkd(nn.Module):\n",
        "  \"\"\"Tnet for k-D point transformation\"\"\"\n",
        "  def __init__(self, k=64):\n",
        "      super(STNkd, self).__init__()\n",
        "      self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.fc3 = nn.Linear(256, k*k)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "      self.k = k\n",
        "\n",
        "  def forward(self, x):\n",
        "      batchsize = x.size()[0]\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = F.relu(self.bn3(self.conv3(x)))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "\n",
        "      x = F.relu(self.bn4(self.fc1(x)))\n",
        "      x = F.relu(self.bn5(self.fc2(x)))\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
        "      if x.is_cuda:\n",
        "          iden = iden.cuda()\n",
        "      x = x + iden\n",
        "      x = x.view(-1, self.k, self.k)\n",
        "      return x\n",
        "\n",
        "class PointNetfeat(nn.Module):\n",
        "  \"\"\"PointNet features part\"\"\"\n",
        "  def __init__(self, global_feat = True, feature_transform = False):\n",
        "      super(PointNetfeat, self).__init__()\n",
        "      self.stn = STN3d()\n",
        "      self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.global_feat = global_feat\n",
        "      self.feature_transform = feature_transform\n",
        "      if self.feature_transform:\n",
        "          self.fstn = STNkd(k=64)\n",
        "\n",
        "  def forward(self, x):\n",
        "      n_pts = x.size()[2]\n",
        "      trans = self.stn(x)\n",
        "      x = x.transpose(2, 1)\n",
        "      x = torch.bmm(x, trans)\n",
        "      x = x.transpose(2, 1)\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "      if self.feature_transform:\n",
        "          trans_feat = self.fstn(x)\n",
        "          x = x.transpose(2,1)\n",
        "          x = torch.bmm(x, trans_feat)\n",
        "          x = x.transpose(2,1)\n",
        "      else:\n",
        "          trans_feat = None\n",
        "\n",
        "      pointfeat = x\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = self.bn3(self.conv3(x))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "      if self.global_feat:\n",
        "          return x, trans, trans_feat\n",
        "      else:\n",
        "          x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "          return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "class DeepCosineMetric(nn.Module):\n",
        "  \"\"\"Deep cosine metric net-end\"\"\"\n",
        "  def __init__(self, k=2, scale_param=1):\n",
        "    super(DeepCosineMetric, self).__init__()\n",
        "    self.fc_last = nn.Linear(256, k, bias=False)\n",
        "    self.scale_param = scale_param\n",
        "  \n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      self.fc_last.weight.div_(torch.norm(self.fc_last.weight, dim=1, keepdim=True))\n",
        "\n",
        "    x = F.normalize(x, p=2, dim=1, eps=1e-12)\n",
        "\n",
        "    x = self.scale_param*self.fc_last(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class PointNetCls(nn.Module):\n",
        "  \"\"\"PointNet for classification\"\"\"\n",
        "  def __init__(self, k=2, feature_transform=False):\n",
        "      super(PointNetCls, self).__init__()\n",
        "      self.feature_transform = feature_transform\n",
        "      self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.deep_cosine_metric = DeepCosineMetric(k=k)\n",
        "\n",
        "      self.dropout = nn.Dropout(p=0.3)\n",
        "      self.bn1 = nn.BatchNorm1d(512)\n",
        "      self.bn2 = nn.BatchNorm1d(256)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x, trans, trans_feat = self.feat(x)\n",
        "      x = F.relu(self.bn1(self.fc1(x)))\n",
        "      x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "      x = self.deep_cosine_metric(x)\n",
        "      return x, trans, trans_feat\n",
        "\n",
        "\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "  \"\"\"Regularization for TNet (TNet transformation matrix should be close to orthogonal)\"\"\"\n",
        "  d = trans.size()[1]\n",
        "  batchsize = trans.size()[0]\n",
        "  I = torch.eye(d)[None, :, :]\n",
        "  if trans.is_cuda:\n",
        "    I = I.to(\"cuda\")\n",
        "  loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHWQPLaEHqVh",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su7CUmUuappM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparams\n",
        "\n",
        "manual_seed = 424242                        # seed for rng\n",
        "cls_num = 2                                 # number of classes, should match that number in dataloader \n",
        "pointnet_feature_transform = True           # Use T-Net block for inner features transformation\n",
        "existing_model_path = \"\"                    # download model state from path (\"\" if no pretrained model to use)\n",
        "epoch_number = 1                            # epoch to train  \n",
        "batch_size = 32                             # should batch match batch size in dataloader\n",
        "val_step = 10                               # evaluate network on validation set example if cur_step % val_step == 0\n",
        "reg_lambda = 0.001                          # coefficient for T-Net orthogonal regularization\n",
        "save_model_to = \"/content/drive/My Drive/PointNet/Model/\"    # path to folder where model will be saved"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSD1TLAf18I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "adam_parameters = {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
        "shelduler_parameters = {\"step_size\": 20, \"gamma\": 0.5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQrWn1B0INlY",
        "colab_type": "code",
        "outputId": "7ec82186-5977-4484-c237-83426926e0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "point_net_cls = PointNetCls(k=cls_num, feature_transform=pointnet_feature_transform)\n",
        "\n",
        "train_loader, val_loader = create_train_val_data_loaders(\"/content/drive/My Drive/PointNet/Data_folder_train_0001/\", \n",
        "                                                         num_of_workers=2)\n",
        "\n",
        "if existing_model_path != \"\":\n",
        "    point_net_cls.load_state_dict(torch.load(existing_model_path))\n",
        "\n",
        "optimizer = optim.Adam(point_net_cls.parameters(), **adam_parameters)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, **shelduler_parameters)\n",
        "\n",
        "point_net_cls.cuda()\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "\n",
        "for epoch in range(epoch_number):\n",
        "    scheduler.step()\n",
        "\n",
        "    for i, [points, target] in enumerate(train_loader):\n",
        "\n",
        "        points = points.squeeze().transpose(2, 1)\n",
        "\n",
        "        points, target = points.to(device, dtype=torch.float), target.to(device) # tensors to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        point_net_cls = point_net_cls.train()\n",
        "\n",
        "        pred, trans, trans_feat = point_net_cls(points)\n",
        "\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        \n",
        "        if pointnet_feature_transform:\n",
        "            loss += feature_transform_regularizer(trans_feat) * reg_lambda\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_choice = pred.data.max(1)[1]\n",
        "\n",
        "        correct = pred_choice.eq(target.data).cpu().sum()\n",
        "\n",
        "        print('[epoch #{0}: {1}/{2}] train loss: {3} batch accuracy: {4}'.format(epoch, i, num_batches, loss.item(), correct.item() / float(batch_size)))\n",
        "\n",
        "        if i % val_step == 0:\n",
        "            j, [points, target] = next(enumerate(val_loader))\n",
        "\n",
        "            points = points.squeeze().transpose(2, 1)\n",
        "\n",
        "            points, target = points.cuda(), target.cuda()\n",
        "\n",
        "            point_net_cls = point_net_cls.eval()\n",
        "\n",
        "            pred, _, _ = point_net_cls(points)\n",
        "\n",
        "            loss = F.nll_loss(pred, target)\n",
        "\n",
        "            pred_choice = pred.data.max(1)[1]\n",
        "\n",
        "            correct = pred_choice.eq(target.data).cpu().sum()\n",
        "\n",
        "            print('[epoch #{0}: {1}/{2}] validation loss: {3} batch accuracy: {4}'.format(epoch, i, num_batches, loss.item(), correct.item()/float(batch_size)))\n",
        "\n",
        "        torch.save(point_net_cls.state_dict(), os.path.join(save_model_to, \"cls_model_{0}.pth\".format(epoch)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch #0: 0/502] train loss: 0.8487129211425781 batch accuracy: 0.375\n",
            "[epoch #0: 0/502] validation loss: 0.8049327731132507 batch accuracy: 0.25\n",
            "[epoch #0: 1/502] train loss: 0.7864579558372498 batch accuracy: 0.625\n",
            "[epoch #0: 2/502] train loss: 0.7720398306846619 batch accuracy: 0.625\n",
            "[epoch #0: 3/502] train loss: 0.7167998552322388 batch accuracy: 0.71875\n",
            "[epoch #0: 4/502] train loss: 0.7652473449707031 batch accuracy: 0.65625\n",
            "[epoch #0: 5/502] train loss: 0.7493324279785156 batch accuracy: 0.625\n",
            "[epoch #0: 6/502] train loss: 0.7548136115074158 batch accuracy: 0.65625\n",
            "[epoch #0: 7/502] train loss: 0.6900687217712402 batch accuracy: 0.8125\n",
            "[epoch #0: 8/502] train loss: 0.7332444190979004 batch accuracy: 0.75\n",
            "[epoch #0: 9/502] train loss: 0.6733198165893555 batch accuracy: 0.75\n",
            "[epoch #0: 10/502] train loss: 0.7535916566848755 batch accuracy: 0.65625\n",
            "[epoch #0: 10/502] validation loss: 0.6300651431083679 batch accuracy: 0.78125\n",
            "[epoch #0: 11/502] train loss: 0.7039129137992859 batch accuracy: 0.65625\n",
            "[epoch #0: 12/502] train loss: 0.5751373767852783 batch accuracy: 0.84375\n",
            "[epoch #0: 13/502] train loss: 0.6414555311203003 batch accuracy: 0.71875\n",
            "[epoch #0: 14/502] train loss: 0.5504702925682068 batch accuracy: 0.78125\n",
            "[epoch #0: 15/502] train loss: 0.6268829703330994 batch accuracy: 0.6875\n",
            "[epoch #0: 16/502] train loss: 0.6452766060829163 batch accuracy: 0.71875\n",
            "[epoch #0: 17/502] train loss: 0.5943139791488647 batch accuracy: 0.71875\n",
            "[epoch #0: 18/502] train loss: 0.59316486120224 batch accuracy: 0.71875\n",
            "[epoch #0: 19/502] train loss: 0.5187880992889404 batch accuracy: 0.78125\n",
            "[epoch #0: 20/502] train loss: 0.4715637266635895 batch accuracy: 0.84375\n",
            "[epoch #0: 20/502] validation loss: 0.6423230767250061 batch accuracy: 0.71875\n",
            "[epoch #0: 21/502] train loss: 0.43031731247901917 batch accuracy: 0.90625\n",
            "[epoch #0: 22/502] train loss: 0.3908804953098297 batch accuracy: 0.9375\n",
            "[epoch #0: 23/502] train loss: 0.3847113251686096 batch accuracy: 0.9375\n",
            "[epoch #0: 24/502] train loss: 0.4091920852661133 batch accuracy: 0.84375\n",
            "[epoch #0: 25/502] train loss: 0.37716352939605713 batch accuracy: 0.875\n",
            "[epoch #0: 26/502] train loss: 0.4041494131088257 batch accuracy: 0.84375\n",
            "[epoch #0: 27/502] train loss: 0.3974880278110504 batch accuracy: 0.84375\n",
            "[epoch #0: 28/502] train loss: 0.32704830169677734 batch accuracy: 0.9375\n",
            "[epoch #0: 29/502] train loss: 0.3788798451423645 batch accuracy: 0.875\n",
            "[epoch #0: 30/502] train loss: 0.3818383812904358 batch accuracy: 0.84375\n",
            "[epoch #0: 30/502] validation loss: 0.7234659790992737 batch accuracy: 0.5\n",
            "[epoch #0: 31/502] train loss: 0.34915414452552795 batch accuracy: 0.9375\n",
            "[epoch #0: 32/502] train loss: 0.2626296281814575 batch accuracy: 1.0\n",
            "[epoch #0: 33/502] train loss: 0.29849106073379517 batch accuracy: 0.96875\n",
            "[epoch #0: 34/502] train loss: 0.20887669920921326 batch accuracy: 1.0\n",
            "[epoch #0: 35/502] train loss: 0.19822536408901215 batch accuracy: 1.0\n",
            "[epoch #0: 36/502] train loss: 0.31245481967926025 batch accuracy: 0.875\n",
            "[epoch #0: 37/502] train loss: 0.22106815874576569 batch accuracy: 1.0\n",
            "[epoch #0: 38/502] train loss: 0.20622815191745758 batch accuracy: 0.96875\n",
            "[epoch #0: 39/502] train loss: 0.22139599919319153 batch accuracy: 1.0\n",
            "[epoch #0: 40/502] train loss: 0.2154323011636734 batch accuracy: 0.96875\n",
            "[epoch #0: 40/502] validation loss: 0.4256220757961273 batch accuracy: 0.75\n",
            "[epoch #0: 41/502] train loss: 0.18519341945648193 batch accuracy: 1.0\n",
            "[epoch #0: 42/502] train loss: 0.18397867679595947 batch accuracy: 0.96875\n",
            "[epoch #0: 43/502] train loss: 0.22408463060855865 batch accuracy: 0.96875\n",
            "[epoch #0: 44/502] train loss: 0.1806376576423645 batch accuracy: 0.96875\n",
            "[epoch #0: 45/502] train loss: 0.27232083678245544 batch accuracy: 0.90625\n",
            "[epoch #0: 46/502] train loss: 0.2903478443622589 batch accuracy: 0.9375\n",
            "[epoch #0: 47/502] train loss: 0.22702471911907196 batch accuracy: 0.9375\n",
            "[epoch #0: 48/502] train loss: 0.23739516735076904 batch accuracy: 0.90625\n",
            "[epoch #0: 49/502] train loss: 0.1900777518749237 batch accuracy: 1.0\n",
            "[epoch #0: 50/502] train loss: 0.17149405181407928 batch accuracy: 0.96875\n",
            "[epoch #0: 50/502] validation loss: 0.14236094057559967 batch accuracy: 0.96875\n",
            "[epoch #0: 51/502] train loss: 0.3463418781757355 batch accuracy: 0.875\n",
            "[epoch #0: 52/502] train loss: 0.14264026284217834 batch accuracy: 1.0\n",
            "[epoch #0: 53/502] train loss: 0.1528131514787674 batch accuracy: 1.0\n",
            "[epoch #0: 54/502] train loss: 0.1840827763080597 batch accuracy: 0.96875\n",
            "[epoch #0: 55/502] train loss: 0.17688140273094177 batch accuracy: 0.96875\n",
            "[epoch #0: 56/502] train loss: 0.12041498720645905 batch accuracy: 1.0\n",
            "[epoch #0: 57/502] train loss: 0.15378765761852264 batch accuracy: 0.96875\n",
            "[epoch #0: 58/502] train loss: 0.177693709731102 batch accuracy: 0.9375\n",
            "[epoch #0: 59/502] train loss: 0.3309156000614166 batch accuracy: 0.90625\n",
            "[epoch #0: 60/502] train loss: 0.12220399081707001 batch accuracy: 1.0\n",
            "[epoch #0: 60/502] validation loss: 0.3094520568847656 batch accuracy: 0.84375\n",
            "[epoch #0: 61/502] train loss: 0.14992453157901764 batch accuracy: 1.0\n",
            "[epoch #0: 62/502] train loss: 0.13141435384750366 batch accuracy: 1.0\n",
            "[epoch #0: 63/502] train loss: 0.38203907012939453 batch accuracy: 0.8125\n",
            "[epoch #0: 64/502] train loss: 0.15982215106487274 batch accuracy: 0.96875\n",
            "[epoch #0: 65/502] train loss: 0.16767427325248718 batch accuracy: 0.9375\n",
            "[epoch #0: 66/502] train loss: 0.1997257024049759 batch accuracy: 0.9375\n",
            "[epoch #0: 67/502] train loss: 0.19326630234718323 batch accuracy: 0.96875\n",
            "[epoch #0: 68/502] train loss: 0.35841017961502075 batch accuracy: 0.9375\n",
            "[epoch #0: 69/502] train loss: 0.15806293487548828 batch accuracy: 0.96875\n",
            "[epoch #0: 70/502] train loss: 0.10743487626314163 batch accuracy: 1.0\n",
            "[epoch #0: 70/502] validation loss: 0.0650722086429596 batch accuracy: 1.0\n",
            "[epoch #0: 71/502] train loss: 0.11293056607246399 batch accuracy: 1.0\n",
            "[epoch #0: 72/502] train loss: 0.17915523052215576 batch accuracy: 0.90625\n",
            "[epoch #0: 73/502] train loss: 0.11635909229516983 batch accuracy: 0.96875\n",
            "[epoch #0: 74/502] train loss: 0.5931232571601868 batch accuracy: 0.78125\n",
            "[epoch #0: 75/502] train loss: 0.23692315816879272 batch accuracy: 0.9375\n",
            "[epoch #0: 76/502] train loss: 0.31994518637657166 batch accuracy: 0.90625\n",
            "[epoch #0: 77/502] train loss: 0.18208450078964233 batch accuracy: 0.96875\n",
            "[epoch #0: 78/502] train loss: 0.22469283640384674 batch accuracy: 1.0\n",
            "[epoch #0: 79/502] train loss: 0.1903637945652008 batch accuracy: 0.9375\n",
            "[epoch #0: 80/502] train loss: 0.14694111049175262 batch accuracy: 1.0\n",
            "[epoch #0: 80/502] validation loss: 0.06319095194339752 batch accuracy: 1.0\n",
            "[epoch #0: 81/502] train loss: 0.16006997227668762 batch accuracy: 1.0\n",
            "[epoch #0: 82/502] train loss: 0.12982212007045746 batch accuracy: 1.0\n",
            "[epoch #0: 83/502] train loss: 0.24197015166282654 batch accuracy: 0.90625\n",
            "[epoch #0: 84/502] train loss: 0.30915409326553345 batch accuracy: 0.9375\n",
            "[epoch #0: 85/502] train loss: 0.2557726502418518 batch accuracy: 0.9375\n",
            "[epoch #0: 86/502] train loss: 0.15516704320907593 batch accuracy: 0.96875\n",
            "[epoch #0: 87/502] train loss: 0.13486501574516296 batch accuracy: 1.0\n",
            "[epoch #0: 88/502] train loss: 0.1415828913450241 batch accuracy: 1.0\n",
            "[epoch #0: 89/502] train loss: 0.17606301605701447 batch accuracy: 0.96875\n",
            "[epoch #0: 90/502] train loss: 0.14398908615112305 batch accuracy: 0.96875\n",
            "[epoch #0: 90/502] validation loss: 0.09327130019664764 batch accuracy: 0.96875\n",
            "[epoch #0: 91/502] train loss: 0.24980534613132477 batch accuracy: 0.9375\n",
            "[epoch #0: 92/502] train loss: 0.1643873006105423 batch accuracy: 0.96875\n",
            "[epoch #0: 93/502] train loss: 0.12969742715358734 batch accuracy: 1.0\n",
            "[epoch #0: 94/502] train loss: 0.16230244934558868 batch accuracy: 0.96875\n",
            "[epoch #0: 95/502] train loss: 0.1674097180366516 batch accuracy: 0.96875\n",
            "[epoch #0: 96/502] train loss: 0.16878250241279602 batch accuracy: 0.96875\n",
            "[epoch #0: 97/502] train loss: 0.10498391091823578 batch accuracy: 1.0\n",
            "[epoch #0: 98/502] train loss: 0.12007385492324829 batch accuracy: 1.0\n",
            "[epoch #0: 99/502] train loss: 0.1940249502658844 batch accuracy: 0.90625\n",
            "[epoch #0: 100/502] train loss: 0.0874854028224945 batch accuracy: 1.0\n",
            "[epoch #0: 100/502] validation loss: 0.047268178313970566 batch accuracy: 1.0\n",
            "[epoch #0: 101/502] train loss: 0.10036664456129074 batch accuracy: 1.0\n",
            "[epoch #0: 102/502] train loss: 0.20017242431640625 batch accuracy: 0.9375\n",
            "[epoch #0: 103/502] train loss: 0.41738462448120117 batch accuracy: 0.84375\n",
            "[epoch #0: 104/502] train loss: 0.15551169216632843 batch accuracy: 0.96875\n",
            "[epoch #0: 105/502] train loss: 0.09665311127901077 batch accuracy: 1.0\n",
            "[epoch #0: 106/502] train loss: 0.11305825412273407 batch accuracy: 1.0\n",
            "[epoch #0: 107/502] train loss: 0.13768693804740906 batch accuracy: 1.0\n",
            "[epoch #0: 108/502] train loss: 0.20152828097343445 batch accuracy: 0.9375\n",
            "[epoch #0: 109/502] train loss: 0.09558670967817307 batch accuracy: 1.0\n",
            "[epoch #0: 110/502] train loss: 0.13321219384670258 batch accuracy: 0.96875\n",
            "[epoch #0: 110/502] validation loss: 0.07679449766874313 batch accuracy: 1.0\n",
            "[epoch #0: 111/502] train loss: 0.09049126505851746 batch accuracy: 1.0\n",
            "[epoch #0: 112/502] train loss: 0.26836907863616943 batch accuracy: 0.90625\n",
            "[epoch #0: 113/502] train loss: 0.20481932163238525 batch accuracy: 0.96875\n",
            "[epoch #0: 114/502] train loss: 0.08099622279405594 batch accuracy: 1.0\n",
            "[epoch #0: 115/502] train loss: 0.12490250915288925 batch accuracy: 0.96875\n",
            "[epoch #0: 116/502] train loss: 0.11443635821342468 batch accuracy: 1.0\n",
            "[epoch #0: 117/502] train loss: 0.10025583952665329 batch accuracy: 1.0\n",
            "[epoch #0: 118/502] train loss: 0.09395703673362732 batch accuracy: 1.0\n",
            "[epoch #0: 119/502] train loss: 0.20184972882270813 batch accuracy: 0.96875\n",
            "[epoch #0: 120/502] train loss: 0.09930881857872009 batch accuracy: 1.0\n",
            "[epoch #0: 120/502] validation loss: 0.08217722177505493 batch accuracy: 0.96875\n",
            "[epoch #0: 121/502] train loss: 0.07694701850414276 batch accuracy: 1.0\n",
            "[epoch #0: 122/502] train loss: 0.11909711360931396 batch accuracy: 0.96875\n",
            "[epoch #0: 123/502] train loss: 0.11468840390443802 batch accuracy: 0.96875\n",
            "[epoch #0: 124/502] train loss: 0.10239064693450928 batch accuracy: 1.0\n",
            "[epoch #0: 125/502] train loss: 0.15350772440433502 batch accuracy: 0.9375\n",
            "[epoch #0: 126/502] train loss: 0.18924963474273682 batch accuracy: 0.96875\n",
            "[epoch #0: 127/502] train loss: 0.1790834218263626 batch accuracy: 0.96875\n",
            "[epoch #0: 128/502] train loss: 0.19622014462947845 batch accuracy: 0.96875\n",
            "[epoch #0: 129/502] train loss: 0.18264633417129517 batch accuracy: 0.96875\n",
            "[epoch #0: 130/502] train loss: 0.0825946256518364 batch accuracy: 1.0\n",
            "[epoch #0: 130/502] validation loss: 0.03312399983406067 batch accuracy: 1.0\n",
            "[epoch #0: 131/502] train loss: 0.13831771910190582 batch accuracy: 0.96875\n",
            "[epoch #0: 132/502] train loss: 0.085851289331913 batch accuracy: 1.0\n",
            "[epoch #0: 133/502] train loss: 0.12615594267845154 batch accuracy: 0.96875\n",
            "[epoch #0: 134/502] train loss: 0.16876348853111267 batch accuracy: 0.9375\n",
            "[epoch #0: 135/502] train loss: 0.18755468726158142 batch accuracy: 0.9375\n",
            "[epoch #0: 136/502] train loss: 0.068901926279068 batch accuracy: 1.0\n",
            "[epoch #0: 137/502] train loss: 0.2648273706436157 batch accuracy: 0.90625\n",
            "[epoch #0: 138/502] train loss: 0.13713878393173218 batch accuracy: 0.96875\n",
            "[epoch #0: 139/502] train loss: 0.2572202682495117 batch accuracy: 0.90625\n",
            "[epoch #0: 140/502] train loss: 0.19807137548923492 batch accuracy: 0.96875\n",
            "[epoch #0: 140/502] validation loss: 0.04682498425245285 batch accuracy: 1.0\n",
            "[epoch #0: 141/502] train loss: 0.1327182501554489 batch accuracy: 0.9375\n",
            "[epoch #0: 142/502] train loss: 0.15039730072021484 batch accuracy: 0.9375\n",
            "[epoch #0: 143/502] train loss: 0.13142718374729156 batch accuracy: 0.96875\n",
            "[epoch #0: 144/502] train loss: 0.06114799901843071 batch accuracy: 1.0\n",
            "[epoch #0: 145/502] train loss: 0.3569779098033905 batch accuracy: 0.90625\n",
            "[epoch #0: 146/502] train loss: 0.16175976395606995 batch accuracy: 0.96875\n",
            "[epoch #0: 147/502] train loss: 0.22014087438583374 batch accuracy: 0.9375\n",
            "[epoch #0: 148/502] train loss: 0.06469954550266266 batch accuracy: 1.0\n",
            "[epoch #0: 149/502] train loss: 0.08856289833784103 batch accuracy: 1.0\n",
            "[epoch #0: 150/502] train loss: 0.11185538023710251 batch accuracy: 0.96875\n",
            "[epoch #0: 150/502] validation loss: 0.13649511337280273 batch accuracy: 0.96875\n",
            "[epoch #0: 151/502] train loss: 0.10930442065000534 batch accuracy: 0.96875\n",
            "[epoch #0: 152/502] train loss: 0.17508403956890106 batch accuracy: 0.9375\n",
            "[epoch #0: 153/502] train loss: 0.07522960007190704 batch accuracy: 1.0\n",
            "[epoch #0: 154/502] train loss: 0.10591371357440948 batch accuracy: 1.0\n",
            "[epoch #0: 155/502] train loss: 0.23591402173042297 batch accuracy: 0.90625\n",
            "[epoch #0: 156/502] train loss: 0.09372235834598541 batch accuracy: 1.0\n",
            "[epoch #0: 157/502] train loss: 0.09162181615829468 batch accuracy: 1.0\n",
            "[epoch #0: 158/502] train loss: 0.11674424260854721 batch accuracy: 0.96875\n",
            "[epoch #0: 159/502] train loss: 0.09166380017995834 batch accuracy: 1.0\n",
            "[epoch #0: 160/502] train loss: 0.2784598469734192 batch accuracy: 0.90625\n",
            "[epoch #0: 160/502] validation loss: 0.08421115577220917 batch accuracy: 0.96875\n",
            "[epoch #0: 161/502] train loss: 0.06839501857757568 batch accuracy: 1.0\n",
            "[epoch #0: 162/502] train loss: 0.06395619362592697 batch accuracy: 1.0\n",
            "[epoch #0: 163/502] train loss: 0.0680304616689682 batch accuracy: 1.0\n",
            "[epoch #0: 164/502] train loss: 0.20438426733016968 batch accuracy: 0.9375\n",
            "[epoch #0: 165/502] train loss: 0.16803328692913055 batch accuracy: 0.96875\n",
            "[epoch #0: 166/502] train loss: 0.11447658389806747 batch accuracy: 0.96875\n",
            "[epoch #0: 167/502] train loss: 0.05778837949037552 batch accuracy: 1.0\n",
            "[epoch #0: 168/502] train loss: 0.17301781475543976 batch accuracy: 0.90625\n",
            "[epoch #0: 169/502] train loss: 0.0889882817864418 batch accuracy: 1.0\n",
            "[epoch #0: 170/502] train loss: 0.16397970914840698 batch accuracy: 0.96875\n",
            "[epoch #0: 170/502] validation loss: 0.09934777766466141 batch accuracy: 0.96875\n",
            "[epoch #0: 171/502] train loss: 0.0691700130701065 batch accuracy: 1.0\n",
            "[epoch #0: 172/502] train loss: 0.14171625673770905 batch accuracy: 0.9375\n",
            "[epoch #0: 173/502] train loss: 0.1899685561656952 batch accuracy: 0.96875\n",
            "[epoch #0: 174/502] train loss: 0.06136426329612732 batch accuracy: 1.0\n",
            "[epoch #0: 175/502] train loss: 0.0845106691122055 batch accuracy: 1.0\n",
            "[epoch #0: 176/502] train loss: 0.1065034568309784 batch accuracy: 0.96875\n",
            "[epoch #0: 177/502] train loss: 0.10301529616117477 batch accuracy: 0.96875\n",
            "[epoch #0: 178/502] train loss: 0.19500769674777985 batch accuracy: 0.9375\n",
            "[epoch #0: 179/502] train loss: 0.06385238468647003 batch accuracy: 1.0\n",
            "[epoch #0: 180/502] train loss: 0.1476871818304062 batch accuracy: 0.9375\n",
            "[epoch #0: 180/502] validation loss: 0.01938454806804657 batch accuracy: 1.0\n",
            "[epoch #0: 181/502] train loss: 0.08737923204898834 batch accuracy: 1.0\n",
            "[epoch #0: 182/502] train loss: 0.08039577305316925 batch accuracy: 1.0\n",
            "[epoch #0: 183/502] train loss: 0.1458120346069336 batch accuracy: 0.96875\n",
            "[epoch #0: 184/502] train loss: 0.08549965173006058 batch accuracy: 1.0\n",
            "[epoch #0: 185/502] train loss: 0.07555815577507019 batch accuracy: 1.0\n",
            "[epoch #0: 186/502] train loss: 0.2229781448841095 batch accuracy: 0.9375\n",
            "[epoch #0: 187/502] train loss: 0.08102331310510635 batch accuracy: 1.0\n",
            "[epoch #0: 188/502] train loss: 0.06816551834344864 batch accuracy: 1.0\n",
            "[epoch #0: 189/502] train loss: 0.16457369923591614 batch accuracy: 0.90625\n",
            "[epoch #0: 190/502] train loss: 0.21796590089797974 batch accuracy: 0.9375\n",
            "[epoch #0: 190/502] validation loss: 0.02893737703561783 batch accuracy: 1.0\n",
            "[epoch #0: 191/502] train loss: 0.08045719563961029 batch accuracy: 0.96875\n",
            "[epoch #0: 192/502] train loss: 0.21713916957378387 batch accuracy: 0.9375\n",
            "[epoch #0: 193/502] train loss: 0.4740764796733856 batch accuracy: 0.875\n",
            "[epoch #0: 194/502] train loss: 0.18985384702682495 batch accuracy: 0.9375\n",
            "[epoch #0: 195/502] train loss: 0.06629537045955658 batch accuracy: 1.0\n",
            "[epoch #0: 196/502] train loss: 0.23234833776950836 batch accuracy: 0.90625\n",
            "[epoch #0: 197/502] train loss: 0.16995513439178467 batch accuracy: 0.96875\n",
            "[epoch #0: 198/502] train loss: 0.1571860909461975 batch accuracy: 0.96875\n",
            "[epoch #0: 199/502] train loss: 0.16746118664741516 batch accuracy: 0.9375\n",
            "[epoch #0: 200/502] train loss: 0.12160199135541916 batch accuracy: 0.96875\n",
            "[epoch #0: 200/502] validation loss: 0.03233066201210022 batch accuracy: 1.0\n",
            "[epoch #0: 201/502] train loss: 0.07203041017055511 batch accuracy: 1.0\n",
            "[epoch #0: 202/502] train loss: 0.26578161120414734 batch accuracy: 0.875\n",
            "[epoch #0: 203/502] train loss: 0.09242254495620728 batch accuracy: 1.0\n",
            "[epoch #0: 204/502] train loss: 0.08278407156467438 batch accuracy: 1.0\n",
            "[epoch #0: 205/502] train loss: 0.108805350959301 batch accuracy: 0.96875\n",
            "[epoch #0: 206/502] train loss: 0.21491733193397522 batch accuracy: 0.875\n",
            "[epoch #0: 207/502] train loss: 0.13796760141849518 batch accuracy: 0.96875\n",
            "[epoch #0: 208/502] train loss: 0.10369186103343964 batch accuracy: 1.0\n",
            "[epoch #0: 209/502] train loss: 0.1006302461028099 batch accuracy: 1.0\n",
            "[epoch #0: 210/502] train loss: 0.15072105824947357 batch accuracy: 0.9375\n",
            "[epoch #0: 210/502] validation loss: 0.05158095434308052 batch accuracy: 1.0\n",
            "[epoch #0: 211/502] train loss: 0.13517257571220398 batch accuracy: 0.96875\n",
            "[epoch #0: 212/502] train loss: 0.07052002847194672 batch accuracy: 1.0\n",
            "[epoch #0: 213/502] train loss: 0.16224545240402222 batch accuracy: 0.90625\n",
            "[epoch #0: 214/502] train loss: 0.18505485355854034 batch accuracy: 0.9375\n",
            "[epoch #0: 215/502] train loss: 0.10231813788414001 batch accuracy: 1.0\n",
            "[epoch #0: 216/502] train loss: 0.05945495888590813 batch accuracy: 1.0\n",
            "[epoch #0: 217/502] train loss: 0.08024874329566956 batch accuracy: 1.0\n",
            "[epoch #0: 218/502] train loss: 0.07377980649471283 batch accuracy: 1.0\n",
            "[epoch #0: 219/502] train loss: 0.09906717389822006 batch accuracy: 1.0\n",
            "[epoch #0: 220/502] train loss: 0.1055753156542778 batch accuracy: 0.96875\n",
            "[epoch #0: 220/502] validation loss: 0.038030531257390976 batch accuracy: 1.0\n",
            "[epoch #0: 221/502] train loss: 0.1024756133556366 batch accuracy: 0.96875\n",
            "[epoch #0: 222/502] train loss: 0.22478723526000977 batch accuracy: 0.9375\n",
            "[epoch #0: 223/502] train loss: 0.11753310263156891 batch accuracy: 0.96875\n",
            "[epoch #0: 224/502] train loss: 0.2254854142665863 batch accuracy: 0.90625\n",
            "[epoch #0: 225/502] train loss: 0.08551417291164398 batch accuracy: 0.96875\n",
            "[epoch #0: 226/502] train loss: 0.1324118673801422 batch accuracy: 0.96875\n",
            "[epoch #0: 227/502] train loss: 0.14699505269527435 batch accuracy: 0.9375\n",
            "[epoch #0: 228/502] train loss: 0.054273176938295364 batch accuracy: 1.0\n",
            "[epoch #0: 229/502] train loss: 0.06648369878530502 batch accuracy: 1.0\n",
            "[epoch #0: 230/502] train loss: 0.13018716871738434 batch accuracy: 0.96875\n",
            "[epoch #0: 230/502] validation loss: 0.02514394000172615 batch accuracy: 1.0\n",
            "[epoch #0: 231/502] train loss: 0.11493417620658875 batch accuracy: 0.96875\n",
            "[epoch #0: 232/502] train loss: 0.119801364839077 batch accuracy: 0.9375\n",
            "[epoch #0: 233/502] train loss: 0.21161451935768127 batch accuracy: 0.90625\n",
            "[epoch #0: 234/502] train loss: 0.11286727339029312 batch accuracy: 0.96875\n",
            "[epoch #0: 235/502] train loss: 0.16023148596286774 batch accuracy: 0.96875\n",
            "[epoch #0: 236/502] train loss: 0.1373569518327713 batch accuracy: 0.9375\n",
            "[epoch #0: 237/502] train loss: 0.08741611242294312 batch accuracy: 0.96875\n",
            "[epoch #0: 238/502] train loss: 0.11013289541006088 batch accuracy: 0.96875\n",
            "[epoch #0: 239/502] train loss: 0.08032255619764328 batch accuracy: 1.0\n",
            "[epoch #0: 240/502] train loss: 0.1817845106124878 batch accuracy: 0.9375\n",
            "[epoch #0: 240/502] validation loss: 0.030245453119277954 batch accuracy: 1.0\n",
            "[epoch #0: 241/502] train loss: 0.16791793704032898 batch accuracy: 0.9375\n",
            "[epoch #0: 242/502] train loss: 0.14232386648654938 batch accuracy: 0.96875\n",
            "[epoch #0: 243/502] train loss: 0.2882402241230011 batch accuracy: 0.9375\n",
            "[epoch #0: 244/502] train loss: 0.055947840213775635 batch accuracy: 1.0\n",
            "[epoch #0: 245/502] train loss: 0.10696817934513092 batch accuracy: 0.96875\n",
            "[epoch #0: 246/502] train loss: 0.10023897886276245 batch accuracy: 1.0\n",
            "[epoch #0: 247/502] train loss: 0.15563209354877472 batch accuracy: 0.96875\n",
            "[epoch #0: 248/502] train loss: 0.07773454487323761 batch accuracy: 1.0\n",
            "[epoch #0: 249/502] train loss: 0.21312251687049866 batch accuracy: 0.96875\n",
            "[epoch #0: 250/502] train loss: 0.07769385725259781 batch accuracy: 1.0\n",
            "[epoch #0: 250/502] validation loss: 0.15984708070755005 batch accuracy: 0.96875\n",
            "[epoch #0: 251/502] train loss: 0.06413985043764114 batch accuracy: 1.0\n",
            "[epoch #0: 252/502] train loss: 0.10887861251831055 batch accuracy: 0.96875\n",
            "[epoch #0: 253/502] train loss: 0.14625509083271027 batch accuracy: 0.96875\n",
            "[epoch #0: 254/502] train loss: 0.2448810338973999 batch accuracy: 0.96875\n",
            "[epoch #0: 255/502] train loss: 0.07022251933813095 batch accuracy: 1.0\n",
            "[epoch #0: 256/502] train loss: 0.33056309819221497 batch accuracy: 0.90625\n",
            "[epoch #0: 257/502] train loss: 0.16660715639591217 batch accuracy: 0.96875\n",
            "[epoch #0: 258/502] train loss: 0.21570315957069397 batch accuracy: 0.9375\n",
            "[epoch #0: 259/502] train loss: 0.06469781696796417 batch accuracy: 1.0\n",
            "[epoch #0: 260/502] train loss: 0.086287721991539 batch accuracy: 1.0\n",
            "[epoch #0: 260/502] validation loss: 0.02588789351284504 batch accuracy: 1.0\n",
            "[epoch #0: 261/502] train loss: 0.3213789165019989 batch accuracy: 0.90625\n",
            "[epoch #0: 262/502] train loss: 0.14401547610759735 batch accuracy: 0.96875\n",
            "[epoch #0: 263/502] train loss: 0.1066490039229393 batch accuracy: 0.96875\n",
            "[epoch #0: 264/502] train loss: 0.060508083552122116 batch accuracy: 1.0\n",
            "[epoch #0: 265/502] train loss: 0.0857762023806572 batch accuracy: 0.96875\n",
            "[epoch #0: 266/502] train loss: 0.05602097511291504 batch accuracy: 1.0\n",
            "[epoch #0: 267/502] train loss: 0.10620028525590897 batch accuracy: 0.96875\n",
            "[epoch #0: 268/502] train loss: 0.07211931049823761 batch accuracy: 1.0\n",
            "[epoch #0: 269/502] train loss: 0.1998673677444458 batch accuracy: 0.9375\n",
            "[epoch #0: 270/502] train loss: 0.06799913197755814 batch accuracy: 1.0\n",
            "[epoch #0: 270/502] validation loss: 0.021514195948839188 batch accuracy: 1.0\n",
            "[epoch #0: 271/502] train loss: 0.07282891869544983 batch accuracy: 1.0\n",
            "[epoch #0: 272/502] train loss: 0.11446301639080048 batch accuracy: 0.96875\n",
            "[epoch #0: 273/502] train loss: 0.06359970569610596 batch accuracy: 1.0\n",
            "[epoch #0: 274/502] train loss: 0.06513075530529022 batch accuracy: 1.0\n",
            "[epoch #0: 275/502] train loss: 0.05957506597042084 batch accuracy: 1.0\n",
            "[epoch #0: 276/502] train loss: 0.05224517732858658 batch accuracy: 1.0\n",
            "[epoch #0: 277/502] train loss: 0.06710030138492584 batch accuracy: 1.0\n",
            "[epoch #0: 278/502] train loss: 0.050019070506095886 batch accuracy: 1.0\n",
            "[epoch #0: 279/502] train loss: 0.1259344071149826 batch accuracy: 0.9375\n",
            "[epoch #0: 280/502] train loss: 0.12343965470790863 batch accuracy: 0.9375\n",
            "[epoch #0: 280/502] validation loss: 0.021994419395923615 batch accuracy: 1.0\n",
            "[epoch #0: 281/502] train loss: 0.046744704246520996 batch accuracy: 1.0\n",
            "[epoch #0: 282/502] train loss: 0.12707209587097168 batch accuracy: 0.96875\n",
            "[epoch #0: 283/502] train loss: 0.21329084038734436 batch accuracy: 0.90625\n",
            "[epoch #0: 284/502] train loss: 0.21018341183662415 batch accuracy: 0.9375\n",
            "[epoch #0: 285/502] train loss: 0.16951699554920197 batch accuracy: 0.96875\n",
            "[epoch #0: 286/502] train loss: 0.05465823411941528 batch accuracy: 1.0\n",
            "[epoch #0: 287/502] train loss: 0.08238081634044647 batch accuracy: 0.96875\n",
            "[epoch #0: 288/502] train loss: 0.046980999410152435 batch accuracy: 1.0\n",
            "[epoch #0: 289/502] train loss: 0.07408753037452698 batch accuracy: 0.96875\n",
            "[epoch #0: 290/502] train loss: 0.06154237687587738 batch accuracy: 1.0\n",
            "[epoch #0: 290/502] validation loss: 0.022810526192188263 batch accuracy: 1.0\n",
            "[epoch #0: 291/502] train loss: 0.05998770520091057 batch accuracy: 1.0\n",
            "[epoch #0: 292/502] train loss: 0.04609597101807594 batch accuracy: 1.0\n",
            "[epoch #0: 293/502] train loss: 0.08487910777330399 batch accuracy: 0.96875\n",
            "[epoch #0: 294/502] train loss: 0.04927700757980347 batch accuracy: 1.0\n",
            "[epoch #0: 295/502] train loss: 0.16996973752975464 batch accuracy: 0.96875\n",
            "[epoch #0: 296/502] train loss: 0.043074026703834534 batch accuracy: 1.0\n",
            "[epoch #0: 297/502] train loss: 0.14010395109653473 batch accuracy: 0.96875\n",
            "[epoch #0: 298/502] train loss: 0.17293725907802582 batch accuracy: 0.9375\n",
            "[epoch #0: 299/502] train loss: 0.06856635957956314 batch accuracy: 0.96875\n",
            "[epoch #0: 300/502] train loss: 0.1728954166173935 batch accuracy: 0.96875\n",
            "[epoch #0: 300/502] validation loss: 0.03862995281815529 batch accuracy: 0.96875\n",
            "[epoch #0: 301/502] train loss: 0.11563186347484589 batch accuracy: 0.96875\n",
            "[epoch #0: 302/502] train loss: 0.1784907728433609 batch accuracy: 0.96875\n",
            "[epoch #0: 303/502] train loss: 0.1742430478334427 batch accuracy: 0.96875\n",
            "[epoch #0: 304/502] train loss: 0.08381259441375732 batch accuracy: 0.96875\n",
            "[epoch #0: 305/502] train loss: 0.09421293437480927 batch accuracy: 0.96875\n",
            "[epoch #0: 306/502] train loss: 0.09399624913930893 batch accuracy: 0.96875\n",
            "[epoch #0: 307/502] train loss: 0.14022216200828552 batch accuracy: 0.96875\n",
            "[epoch #0: 308/502] train loss: 0.073765829205513 batch accuracy: 0.96875\n",
            "[epoch #0: 309/502] train loss: 0.21006137132644653 batch accuracy: 0.9375\n",
            "[epoch #0: 310/502] train loss: 0.04213928431272507 batch accuracy: 1.0\n",
            "[epoch #0: 310/502] validation loss: 0.04203067347407341 batch accuracy: 1.0\n",
            "[epoch #0: 311/502] train loss: 0.2046980857849121 batch accuracy: 0.9375\n",
            "[epoch #0: 312/502] train loss: 0.03975222632288933 batch accuracy: 1.0\n",
            "[epoch #0: 313/502] train loss: 0.044538430869579315 batch accuracy: 1.0\n",
            "[epoch #0: 314/502] train loss: 0.04291881248354912 batch accuracy: 1.0\n",
            "[epoch #0: 315/502] train loss: 0.0891168862581253 batch accuracy: 0.96875\n",
            "[epoch #0: 316/502] train loss: 0.06760045886039734 batch accuracy: 0.96875\n",
            "[epoch #0: 317/502] train loss: 0.04402546212077141 batch accuracy: 1.0\n",
            "[epoch #0: 318/502] train loss: 0.08531276136636734 batch accuracy: 0.96875\n",
            "[epoch #0: 319/502] train loss: 0.07575412839651108 batch accuracy: 0.96875\n",
            "[epoch #0: 320/502] train loss: 0.09111987799406052 batch accuracy: 0.96875\n",
            "[epoch #0: 320/502] validation loss: 0.08571630716323853 batch accuracy: 0.96875\n",
            "[epoch #0: 321/502] train loss: 0.2782239019870758 batch accuracy: 0.90625\n",
            "[epoch #0: 322/502] train loss: 0.0958634614944458 batch accuracy: 0.96875\n",
            "[epoch #0: 323/502] train loss: 0.04667172580957413 batch accuracy: 1.0\n",
            "[epoch #0: 324/502] train loss: 0.12358478456735611 batch accuracy: 0.96875\n",
            "[epoch #0: 325/502] train loss: 0.05962691828608513 batch accuracy: 1.0\n",
            "[epoch #0: 326/502] train loss: 0.13978096842765808 batch accuracy: 0.9375\n",
            "[epoch #0: 327/502] train loss: 0.11760789901018143 batch accuracy: 0.9375\n",
            "[epoch #0: 328/502] train loss: 0.0625528022646904 batch accuracy: 1.0\n",
            "[epoch #0: 329/502] train loss: 0.13448059558868408 batch accuracy: 0.96875\n",
            "[epoch #0: 330/502] train loss: 0.20105911791324615 batch accuracy: 0.875\n",
            "[epoch #0: 330/502] validation loss: 0.02777549996972084 batch accuracy: 1.0\n",
            "[epoch #0: 331/502] train loss: 0.11510570347309113 batch accuracy: 0.9375\n",
            "[epoch #0: 332/502] train loss: 0.0481562502682209 batch accuracy: 1.0\n",
            "[epoch #0: 333/502] train loss: 0.05944032222032547 batch accuracy: 1.0\n",
            "[epoch #0: 334/502] train loss: 0.06928397715091705 batch accuracy: 1.0\n",
            "[epoch #0: 335/502] train loss: 0.06339017301797867 batch accuracy: 1.0\n",
            "[epoch #0: 336/502] train loss: 0.04235200211405754 batch accuracy: 1.0\n",
            "[epoch #0: 337/502] train loss: 0.045764852315187454 batch accuracy: 1.0\n",
            "[epoch #0: 338/502] train loss: 0.04466405510902405 batch accuracy: 1.0\n",
            "[epoch #0: 339/502] train loss: 0.04379229620099068 batch accuracy: 1.0\n",
            "[epoch #0: 340/502] train loss: 0.05950874835252762 batch accuracy: 1.0\n",
            "[epoch #0: 340/502] validation loss: 0.02514071762561798 batch accuracy: 1.0\n",
            "[epoch #0: 341/502] train loss: 0.10483726859092712 batch accuracy: 0.96875\n",
            "[epoch #0: 342/502] train loss: 0.10835565626621246 batch accuracy: 0.96875\n",
            "[epoch #0: 343/502] train loss: 0.2183084487915039 batch accuracy: 0.9375\n",
            "[epoch #0: 344/502] train loss: 0.0351942740380764 batch accuracy: 1.0\n",
            "[epoch #0: 345/502] train loss: 0.09028977900743484 batch accuracy: 0.96875\n",
            "[epoch #0: 346/502] train loss: 0.1950569748878479 batch accuracy: 0.9375\n",
            "[epoch #0: 347/502] train loss: 0.11237848550081253 batch accuracy: 0.96875\n",
            "[epoch #0: 348/502] train loss: 0.08189167827367783 batch accuracy: 0.96875\n",
            "[epoch #0: 349/502] train loss: 0.06002582237124443 batch accuracy: 1.0\n",
            "[epoch #0: 350/502] train loss: 0.28375476598739624 batch accuracy: 0.875\n",
            "[epoch #0: 350/502] validation loss: 0.11680258810520172 batch accuracy: 0.96875\n",
            "[epoch #0: 351/502] train loss: 0.07595611363649368 batch accuracy: 0.96875\n",
            "[epoch #0: 352/502] train loss: 0.08461686968803406 batch accuracy: 1.0\n",
            "[epoch #0: 353/502] train loss: 0.09395471215248108 batch accuracy: 1.0\n",
            "[epoch #0: 354/502] train loss: 0.14540036022663116 batch accuracy: 0.96875\n",
            "[epoch #0: 355/502] train loss: 0.055221281945705414 batch accuracy: 1.0\n",
            "[epoch #0: 356/502] train loss: 0.09020942449569702 batch accuracy: 0.96875\n",
            "[epoch #0: 357/502] train loss: 0.10167872905731201 batch accuracy: 0.96875\n",
            "[epoch #0: 358/502] train loss: 0.06522123515605927 batch accuracy: 1.0\n",
            "[epoch #0: 359/502] train loss: 0.04780477657914162 batch accuracy: 1.0\n",
            "[epoch #0: 360/502] train loss: 0.0626305416226387 batch accuracy: 1.0\n",
            "[epoch #0: 360/502] validation loss: 0.12557825446128845 batch accuracy: 0.96875\n",
            "[epoch #0: 361/502] train loss: 0.04155704751610756 batch accuracy: 1.0\n",
            "[epoch #0: 362/502] train loss: 0.1381644904613495 batch accuracy: 0.96875\n",
            "[epoch #0: 363/502] train loss: 0.05013088881969452 batch accuracy: 1.0\n",
            "[epoch #0: 364/502] train loss: 0.08234976977109909 batch accuracy: 0.96875\n",
            "[epoch #0: 365/502] train loss: 0.08140436559915543 batch accuracy: 0.96875\n",
            "[epoch #0: 366/502] train loss: 0.056774888187646866 batch accuracy: 1.0\n",
            "[epoch #0: 367/502] train loss: 0.20100708305835724 batch accuracy: 0.90625\n",
            "[epoch #0: 368/502] train loss: 0.038192275911569595 batch accuracy: 1.0\n",
            "[epoch #0: 369/502] train loss: 0.1327366828918457 batch accuracy: 0.96875\n",
            "[epoch #0: 370/502] train loss: 0.08318129926919937 batch accuracy: 0.96875\n",
            "[epoch #0: 370/502] validation loss: 0.022058766335248947 batch accuracy: 1.0\n",
            "[epoch #0: 371/502] train loss: 0.053750716149806976 batch accuracy: 1.0\n",
            "[epoch #0: 372/502] train loss: 0.04630909860134125 batch accuracy: 1.0\n",
            "[epoch #0: 373/502] train loss: 0.07862576842308044 batch accuracy: 0.96875\n",
            "[epoch #0: 374/502] train loss: 0.12315984070301056 batch accuracy: 0.96875\n",
            "[epoch #0: 375/502] train loss: 0.18721260130405426 batch accuracy: 0.9375\n",
            "[epoch #0: 376/502] train loss: 0.1617840826511383 batch accuracy: 0.96875\n",
            "[epoch #0: 377/502] train loss: 0.06763222068548203 batch accuracy: 1.0\n",
            "[epoch #0: 378/502] train loss: 0.052058082073926926 batch accuracy: 1.0\n",
            "[epoch #0: 379/502] train loss: 0.06399255245923996 batch accuracy: 0.96875\n",
            "[epoch #0: 380/502] train loss: 0.08566418290138245 batch accuracy: 0.96875\n",
            "[epoch #0: 380/502] validation loss: 0.051143813878297806 batch accuracy: 0.96875\n",
            "[epoch #0: 381/502] train loss: 0.05262492224574089 batch accuracy: 1.0\n",
            "[epoch #0: 382/502] train loss: 0.1137625202536583 batch accuracy: 0.96875\n",
            "[epoch #0: 383/502] train loss: 0.046256765723228455 batch accuracy: 1.0\n",
            "[epoch #0: 384/502] train loss: 0.08959192037582397 batch accuracy: 0.96875\n",
            "[epoch #0: 385/502] train loss: 0.04935125261545181 batch accuracy: 1.0\n",
            "[epoch #0: 386/502] train loss: 0.07464342564344406 batch accuracy: 0.96875\n",
            "[epoch #0: 387/502] train loss: 0.12330800294876099 batch accuracy: 0.96875\n",
            "[epoch #0: 388/502] train loss: 0.06693942099809647 batch accuracy: 1.0\n",
            "[epoch #0: 389/502] train loss: 0.05111974477767944 batch accuracy: 1.0\n",
            "[epoch #0: 390/502] train loss: 0.10162870585918427 batch accuracy: 0.96875\n",
            "[epoch #0: 390/502] validation loss: 0.07599987089633942 batch accuracy: 0.96875\n",
            "[epoch #0: 391/502] train loss: 0.03711015731096268 batch accuracy: 1.0\n",
            "[epoch #0: 392/502] train loss: 0.057616304606199265 batch accuracy: 1.0\n",
            "[epoch #0: 393/502] train loss: 0.21051032841205597 batch accuracy: 0.9375\n",
            "[epoch #0: 394/502] train loss: 0.040398091077804565 batch accuracy: 1.0\n",
            "[epoch #0: 395/502] train loss: 0.06295691430568695 batch accuracy: 1.0\n",
            "[epoch #0: 396/502] train loss: 0.032340049743652344 batch accuracy: 1.0\n",
            "[epoch #0: 397/502] train loss: 0.03692345321178436 batch accuracy: 1.0\n",
            "[epoch #0: 398/502] train loss: 0.03555449843406677 batch accuracy: 1.0\n",
            "[epoch #0: 399/502] train loss: 0.1004796177148819 batch accuracy: 0.96875\n",
            "[epoch #0: 400/502] train loss: 0.034797634929418564 batch accuracy: 1.0\n",
            "[epoch #0: 400/502] validation loss: 0.014777082949876785 batch accuracy: 1.0\n",
            "[epoch #0: 401/502] train loss: 0.1179562658071518 batch accuracy: 0.96875\n",
            "[epoch #0: 402/502] train loss: 0.13898278772830963 batch accuracy: 0.9375\n",
            "[epoch #0: 403/502] train loss: 0.04129007086157799 batch accuracy: 1.0\n",
            "[epoch #0: 404/502] train loss: 0.18459534645080566 batch accuracy: 0.9375\n",
            "[epoch #0: 405/502] train loss: 0.037225689738988876 batch accuracy: 1.0\n",
            "[epoch #0: 406/502] train loss: 0.14424629509449005 batch accuracy: 0.9375\n",
            "[epoch #0: 407/502] train loss: 0.08971099555492401 batch accuracy: 0.96875\n",
            "[epoch #0: 408/502] train loss: 0.19378094375133514 batch accuracy: 0.9375\n",
            "[epoch #0: 409/502] train loss: 0.13364391028881073 batch accuracy: 0.9375\n",
            "[epoch #0: 410/502] train loss: 0.04824249446392059 batch accuracy: 1.0\n",
            "[epoch #0: 410/502] validation loss: 0.016757730394601822 batch accuracy: 1.0\n",
            "[epoch #0: 411/502] train loss: 0.04339207708835602 batch accuracy: 1.0\n",
            "[epoch #0: 412/502] train loss: 0.040579359978437424 batch accuracy: 1.0\n",
            "[epoch #0: 413/502] train loss: 0.05177159607410431 batch accuracy: 1.0\n",
            "[epoch #0: 414/502] train loss: 0.06602590531110764 batch accuracy: 0.96875\n",
            "[epoch #0: 415/502] train loss: 0.04149430990219116 batch accuracy: 1.0\n",
            "[epoch #0: 416/502] train loss: 0.4372684955596924 batch accuracy: 0.875\n",
            "[epoch #0: 417/502] train loss: 0.05625297129154205 batch accuracy: 0.96875\n",
            "[epoch #0: 418/502] train loss: 0.03274795785546303 batch accuracy: 1.0\n",
            "[epoch #0: 419/502] train loss: 0.1130111962556839 batch accuracy: 0.96875\n",
            "[epoch #0: 420/502] train loss: 0.16695517301559448 batch accuracy: 0.96875\n",
            "[epoch #0: 420/502] validation loss: 0.01839977502822876 batch accuracy: 1.0\n",
            "[epoch #0: 421/502] train loss: 0.11760758608579636 batch accuracy: 0.96875\n",
            "[epoch #0: 422/502] train loss: 0.11124160885810852 batch accuracy: 0.9375\n",
            "[epoch #0: 423/502] train loss: 0.04006655141711235 batch accuracy: 1.0\n",
            "[epoch #0: 424/502] train loss: 0.11056697368621826 batch accuracy: 0.9375\n",
            "[epoch #0: 425/502] train loss: 0.053496651351451874 batch accuracy: 1.0\n",
            "[epoch #0: 426/502] train loss: 0.11453871428966522 batch accuracy: 0.96875\n",
            "[epoch #0: 427/502] train loss: 0.13930945098400116 batch accuracy: 0.9375\n",
            "[epoch #0: 428/502] train loss: 0.32735294103622437 batch accuracy: 0.8125\n",
            "[epoch #0: 429/502] train loss: 0.14560966193675995 batch accuracy: 0.9375\n",
            "[epoch #0: 430/502] train loss: 0.0612495020031929 batch accuracy: 1.0\n",
            "[epoch #0: 430/502] validation loss: 0.01952371373772621 batch accuracy: 1.0\n",
            "[epoch #0: 431/502] train loss: 0.05275701358914375 batch accuracy: 1.0\n",
            "[epoch #0: 432/502] train loss: 0.12330817431211472 batch accuracy: 0.96875\n",
            "[epoch #0: 433/502] train loss: 0.18411043286323547 batch accuracy: 0.96875\n",
            "[epoch #0: 434/502] train loss: 0.18318751454353333 batch accuracy: 0.96875\n",
            "[epoch #0: 435/502] train loss: 0.06726193428039551 batch accuracy: 1.0\n",
            "[epoch #0: 436/502] train loss: 0.04606477916240692 batch accuracy: 1.0\n",
            "[epoch #0: 437/502] train loss: 0.1485072374343872 batch accuracy: 0.96875\n",
            "[epoch #0: 438/502] train loss: 0.11927776038646698 batch accuracy: 0.96875\n",
            "[epoch #0: 439/502] train loss: 0.05444984883069992 batch accuracy: 1.0\n",
            "[epoch #0: 440/502] train loss: 0.06106167286634445 batch accuracy: 1.0\n",
            "[epoch #0: 440/502] validation loss: 0.021279245615005493 batch accuracy: 1.0\n",
            "[epoch #0: 441/502] train loss: 0.14305758476257324 batch accuracy: 0.96875\n",
            "[epoch #0: 442/502] train loss: 0.09065195173025131 batch accuracy: 0.96875\n",
            "[epoch #0: 443/502] train loss: 0.06270711123943329 batch accuracy: 1.0\n",
            "[epoch #0: 444/502] train loss: 0.06198173388838768 batch accuracy: 1.0\n",
            "[epoch #0: 445/502] train loss: 0.0901293084025383 batch accuracy: 0.96875\n",
            "[epoch #0: 446/502] train loss: 0.058392416685819626 batch accuracy: 1.0\n",
            "[epoch #0: 447/502] train loss: 0.1886349320411682 batch accuracy: 0.96875\n",
            "[epoch #0: 448/502] train loss: 0.4434594511985779 batch accuracy: 0.875\n",
            "[epoch #0: 449/502] train loss: 0.14936919510364532 batch accuracy: 0.9375\n",
            "[epoch #0: 450/502] train loss: 0.04956803843379021 batch accuracy: 1.0\n",
            "[epoch #0: 450/502] validation loss: 0.017304223030805588 batch accuracy: 1.0\n",
            "[epoch #0: 451/502] train loss: 0.03879574313759804 batch accuracy: 1.0\n",
            "[epoch #0: 452/502] train loss: 0.1037546917796135 batch accuracy: 0.96875\n",
            "[epoch #0: 453/502] train loss: 0.04382574185729027 batch accuracy: 1.0\n",
            "[epoch #0: 454/502] train loss: 0.03852016106247902 batch accuracy: 1.0\n",
            "[epoch #0: 455/502] train loss: 0.056517988443374634 batch accuracy: 1.0\n",
            "[epoch #0: 456/502] train loss: 0.1469172090291977 batch accuracy: 0.9375\n",
            "[epoch #0: 457/502] train loss: 0.04450834542512894 batch accuracy: 1.0\n",
            "[epoch #0: 458/502] train loss: 0.1765584498643875 batch accuracy: 0.9375\n",
            "[epoch #0: 459/502] train loss: 0.13074243068695068 batch accuracy: 0.96875\n",
            "[epoch #0: 460/502] train loss: 0.05305515229701996 batch accuracy: 1.0\n",
            "[epoch #0: 460/502] validation loss: 0.04798530414700508 batch accuracy: 0.96875\n",
            "[epoch #0: 461/502] train loss: 0.06809487193822861 batch accuracy: 1.0\n",
            "[epoch #0: 462/502] train loss: 0.0399879589676857 batch accuracy: 1.0\n",
            "[epoch #0: 463/502] train loss: 0.3810754716396332 batch accuracy: 0.90625\n",
            "[epoch #0: 464/502] train loss: 0.03621075302362442 batch accuracy: 1.0\n",
            "[epoch #0: 465/502] train loss: 0.08103688061237335 batch accuracy: 1.0\n",
            "[epoch #0: 466/502] train loss: 0.13538868725299835 batch accuracy: 0.9375\n",
            "[epoch #0: 467/502] train loss: 0.07044680416584015 batch accuracy: 0.96875\n",
            "[epoch #0: 468/502] train loss: 0.07230450212955475 batch accuracy: 1.0\n",
            "[epoch #0: 469/502] train loss: 0.25674206018447876 batch accuracy: 0.90625\n",
            "[epoch #0: 470/502] train loss: 0.12298674881458282 batch accuracy: 0.96875\n",
            "[epoch #0: 470/502] validation loss: 0.018753480166196823 batch accuracy: 1.0\n",
            "[epoch #0: 471/502] train loss: 0.08456257730722427 batch accuracy: 0.96875\n",
            "[epoch #0: 472/502] train loss: 0.3249290883541107 batch accuracy: 0.84375\n",
            "[epoch #0: 473/502] train loss: 0.09915038198232651 batch accuracy: 0.96875\n",
            "[epoch #0: 474/502] train loss: 0.1434752345085144 batch accuracy: 0.96875\n",
            "[epoch #0: 475/502] train loss: 0.13203707337379456 batch accuracy: 0.96875\n",
            "[epoch #0: 476/502] train loss: 0.052400507032871246 batch accuracy: 1.0\n",
            "[epoch #0: 477/502] train loss: 0.095921091735363 batch accuracy: 0.96875\n",
            "[epoch #0: 478/502] train loss: 0.0511292926967144 batch accuracy: 1.0\n",
            "[epoch #0: 479/502] train loss: 0.1573278307914734 batch accuracy: 0.9375\n",
            "[epoch #0: 480/502] train loss: 0.04792375490069389 batch accuracy: 1.0\n",
            "[epoch #0: 480/502] validation loss: 0.08538403362035751 batch accuracy: 0.9375\n",
            "[epoch #0: 481/502] train loss: 0.12548768520355225 batch accuracy: 0.9375\n",
            "[epoch #0: 482/502] train loss: 0.07783836871385574 batch accuracy: 0.96875\n",
            "[epoch #0: 483/502] train loss: 0.06544110924005508 batch accuracy: 0.96875\n",
            "[epoch #0: 484/502] train loss: 0.1382131278514862 batch accuracy: 0.90625\n",
            "[epoch #0: 485/502] train loss: 0.08450339734554291 batch accuracy: 0.96875\n",
            "[epoch #0: 486/502] train loss: 0.18673914670944214 batch accuracy: 0.9375\n",
            "[epoch #0: 487/502] train loss: 0.06521637737751007 batch accuracy: 1.0\n",
            "[epoch #0: 488/502] train loss: 0.05319465324282646 batch accuracy: 1.0\n",
            "[epoch #0: 489/502] train loss: 0.16086259484291077 batch accuracy: 0.90625\n",
            "[epoch #0: 490/502] train loss: 0.06900522857904434 batch accuracy: 1.0\n",
            "[epoch #0: 490/502] validation loss: 0.06465902179479599 batch accuracy: 0.96875\n",
            "[epoch #0: 491/502] train loss: 0.04929881542921066 batch accuracy: 1.0\n",
            "[epoch #0: 492/502] train loss: 0.1540299654006958 batch accuracy: 0.96875\n",
            "[epoch #0: 493/502] train loss: 0.10248289257287979 batch accuracy: 0.96875\n",
            "[epoch #0: 494/502] train loss: 0.05561868101358414 batch accuracy: 1.0\n",
            "[epoch #0: 495/502] train loss: 0.09313943982124329 batch accuracy: 0.96875\n",
            "[epoch #0: 496/502] train loss: 0.28735679388046265 batch accuracy: 0.9375\n",
            "[epoch #0: 497/502] train loss: 0.14494764804840088 batch accuracy: 0.96875\n",
            "[epoch #0: 498/502] train loss: 0.14512276649475098 batch accuracy: 0.96875\n",
            "[epoch #0: 499/502] train loss: 0.12722796201705933 batch accuracy: 0.9375\n",
            "[epoch #0: 500/502] train loss: 0.06989069283008575 batch accuracy: 1.0\n",
            "[epoch #0: 500/502] validation loss: 0.05730342119932175 batch accuracy: 0.96875\n",
            "[epoch #0: 501/502] train loss: 0.32953694462776184 batch accuracy: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}